{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading json\n",
    "\n",
    "Get the What's Cooking? json files and organize them in a big spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# starting up a console attached to this kernel\n",
    "%qtconsole\n",
    "import os\n",
    "\n",
    "# importing base code\n",
    "os.chdir('/your-path/whats-cooking/code')\n",
    "from base import *\n",
    "\n",
    "# changing to competition dir\n",
    "os.chdir('/your-path/whats-cooking/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading files\n",
    "# also using stemmed data\n",
    "path = './raw-data'\n",
    "train = pd.read_json(path + '/train.json')\n",
    "test = pd.read_json(path + '/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files are organized as {label, id, [list of ingredients]}.\n",
    "We want to transform this to {id, label, ing1, ing2, ing3,...} like a one-hot encoding.\n",
    "First, let us build a dictionary of ingredients (so we can use tf or tf-idf later). Also let us record the frequency of the features. Finally, this dict must use training data only, to avoid 0-variance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform to big spreadsheet\n",
    "count = 0\n",
    "lengths = []\n",
    "ing_dict = {}\n",
    "freq_dict = {}\n",
    "# writing one column per ingredient\n",
    "# iterating over rows is bad practice, but this is a small dataset\n",
    "for row, data in train.iterrows():\n",
    "    lengths.append(len(ing_dict))\n",
    "    for ingredient in data['ingredients']:\n",
    "        try:\n",
    "            ing_dict[ingredient]\n",
    "            freq_dict[ingredient] += 1\n",
    "        except KeyError:\n",
    "            ing_dict[ingredient] = count\n",
    "            freq_dict[ingredient] = 1\n",
    "            count += 1\n",
    "\n",
    "# ordering the dict \n",
    "import operator\n",
    "sorted_freqs = sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize output:\n",
    "print ing_dict.keys()[0:10]\n",
    "print 'number of ingredients:', len(ing_dict)\n",
    "print 'Top 25 ingredients:'\n",
    "print sorted_freqs[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 6714 ingredients (raw data) is a lot. There must be some overlap (modifiers, typos, etc).\n",
    "\n",
    "Now, let us save our dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving data\n",
    "with open(path + '/ing_dict.txt', 'w') as f:\n",
    "    f.write(str(ing_dict))\n",
    "    \n",
    "with open(path + '/freq_dict.txt', 'w') as f:\n",
    "    f.write(str(freq_dict))\n",
    "    \n",
    "with open(path + '/freq_sorted.txt', 'w') as f:\n",
    "    f.write(str(sorted_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved as literal (not good practice?). To read it, just use ast module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read dict literal\n",
    "with open(path + '/ing_dict.txt', 'r') as f:\n",
    "    ing_dict = ast.literal_eval(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use feature extraction tools from sklearn to build sparse features from our rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a spreadsheet where columns are frequency counts\n",
    "# dummy function, as we want to override the sklearn analyser\n",
    "# and use what is inside the existing lists as tokens\n",
    "do_nothing = lambda x: x \n",
    "\n",
    "# this instance will count the word's frequencies\n",
    "cvect = CountVectorizer(analyzer=do_nothing,\n",
    "                        vocabulary=ing_dict)\n",
    "# getting corpus\n",
    "combi = pd.concat([train, test])\n",
    "corpus = combi['ingredients']\n",
    "\n",
    "# build count matrix\n",
    "counts = cvect.transform(corpus)\n",
    "\n",
    "# turn sparse numpy into pd.DataFrame\n",
    "counts_df = pd.DataFrame(counts.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Moving on: feature extraction, exploratory analysis and feature engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
