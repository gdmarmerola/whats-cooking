{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and model selection\n",
    "\n",
    "Perform hyperparameter optimization and model selection using the raw competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# starting up a console attached to this kernel\n",
    "%qtconsole\n",
    "import os\n",
    "\n",
    "# importing base code\n",
    "os.chdir('/your-path/whats-cooking/code')\n",
    "from base import *\n",
    "\n",
    "# changing to competition dir\n",
    "os.chdir('/your-path/whats-cooking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Framework\n",
    "\n",
    "Here we define metrics, CV type (number of folds and repetitions), etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whats cooking CV framework\n",
    "def wc_framework(space):\n",
    "\n",
    "    # load data\n",
    "    global train\n",
    "    df = copy.deepcopy(train)\n",
    "    global y_train\n",
    "    y = copy.deepcopy(y_train)\n",
    "\n",
    "    # copy the search space\n",
    "    space_copy = copy.deepcopy(space)\n",
    "\n",
    "    # preprocessing with some parameters\n",
    "    preproc_steps = [None, space['feat_sel'].pop('alg')(**space.pop('feat_sel')['args'])]\n",
    "    space.pop('scaling')\n",
    "\n",
    "    # framework with preprocessing and algo\n",
    "    fmwk = supervised_framework(preproc_steps, space.pop('type'), space)\n",
    "    fmwk_copy = copy.deepcopy(fmwk)\n",
    "\n",
    "    # evaluation function choice\n",
    "    acc_fn = metrics.accuracy_score\n",
    "    report_fn = metrics.classification_report\n",
    "\n",
    "    # number of repetitions and folds\n",
    "    n = 1\n",
    "    k = 5\n",
    "\n",
    "    global eval_number\n",
    "    eval_number += 1\n",
    "    print 'eval_number:', eval_number, space_copy\n",
    "\n",
    "    # repeat n times a k-fold cross-val\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "\n",
    "        res.append(strat_k_fold_cross_val(k, df, y, fmwk))\n",
    "\n",
    "    # evaluating...\n",
    "    accuracy_list = []\n",
    "    for results in res:\n",
    "        for key in results.keys():\n",
    "            predictions = results[key]['out']['preds']\n",
    "            y = results[key]['gtruth']\n",
    "            accuracy_list.append(acc_fn(y, predictions))\n",
    "\n",
    "    mean_acc = np.mean(accuracy_list)\n",
    "\n",
    "    print 'accuracy:', mean_acc, 'std:', np.std(accuracy_list)\n",
    "\n",
    "    return {'loss': 1 - mean_acc,\n",
    "            'accuracy': mean_acc,\n",
    "            'acc_sd': np.std(accuracy_list),\n",
    "            'status': STATUS_OK,\n",
    "            'space': space_copy,\n",
    "            'fmwk': fmwk_copy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading data\n",
    "path = './eng-data/stemmed-word2'\n",
    "gtruth = pd.read_json('./raw-data/train.json')['cuisine']\n",
    "train_df = load_sparse_csr(path + '/train.npz')\n",
    "\n",
    "# encode ground truth\n",
    "with open('./raw-data/enc_dict.txt', 'r') as f:\n",
    "    enc_dict = ast.literal_eval(f.read())\n",
    "gtruth = gtruth.replace(enc_dict)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Experiments with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy data\n",
    "train = copy.deepcopy(train_df)\n",
    "y_train = copy.deepcopy(gtruth)\n",
    "\n",
    "# smaller subset for experiments?\n",
    "\n",
    "# dimensionality reduction\n",
    "#svd_dimred = TruncatedSVD(n_components=500)\n",
    "#train = svd_dimred.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with SGD: single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgdsvm_space = {'type': SGDClassifier,\n",
    "                'loss': hp.choice('sgdsvm_loss', ['hinge']),\n",
    "                'penalty': hp.choice('sgdsvm_pen', ['l2']),\n",
    "                'alpha': hp.uniform('sgdsvm_alpha', 0.01, 0.001),\n",
    "                'n_iter': hp.quniform('sgdsvm_iter', 5, 25, 5),\n",
    "                'l1_ratio': hp.uniform('sgdsvm_l1r', 0.01, 1),\n",
    "                'eta0': hp.uniform('sgdsvm_eta0', 0.001, 0.1),\n",
    "                'learning_rate': hp.choice('sgdsvm_lr', ['constant', 'optimal', 'invscaling']),\n",
    "                'class_weight': hp.choice('sgdsvm_cw', ['auto', None]),\n",
    "                'scaling': hp.choice('sgdsvm_scaling', [None, StandardScaler(),\n",
    "                                                     MinMaxScaler(),\n",
    "                                                     MinMaxScaler(feature_range=(-1, 1))]),\n",
    "                'feat_sel': {'alg': SelectPercentile,\n",
    "                             'args': {'score_func': chi2,\n",
    "                                      'percentile': 100}\n",
    "                            }\n",
    "                }\n",
    "\n",
    "\n",
    "# support vector machines\n",
    "eval_number = 0\n",
    "trials = Trials()\n",
    "best_svm = optimize(wc_framework, sgdsvm_space, 15, trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with SGD: boosted ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# boosting the best svm:\n",
    "#algo = joblib.load('./models/sub3.pkl') \n",
    "algo = best_svm['result']['fmwk'].algo\n",
    "\n",
    "adsvm_space = {'type': AdaBoostClassifier,\n",
    "               'base_estimator': algo,\n",
    "               'algorithm': 'SAMME',\n",
    "               'n_estimators': hp.choice('n_estimators_abt', [10, 50]),\n",
    "               'scaling': None\n",
    "               }\n",
    "\n",
    "eval_number = 0\n",
    "trials = Trials()\n",
    "best_adsvm = optimize(wc_framework, sgdsvm_space, 15, trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with SGD: bagged ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bagging the best svm:\n",
    "#algo = joblib.load('./models/sub3.pkl')\n",
    "algo = best_svm['result']['fmwk'].algo\n",
    "\n",
    "bagsvm_space = {'type': BaggingClassifier,\n",
    "                'base_estimator': algo,\n",
    "                'n_estimators': hp.choice('n_estimators_bg', [10, 50, 100]),\n",
    "                'scaling': hp.choice('scaling_bg', [None]),\n",
    "                'max_features': hp.uniform('mf_bg', 0.1, 1),\n",
    "                'max_samples': hp.uniform('ms_bg', 0.1, 1)\n",
    "                }\n",
    "\n",
    "eval_number = 0\n",
    "trials = Trials()\n",
    "best_bagsvm = optimize(wc_framework, bagsvm_space, 15, trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with SGD: single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# logistic regression with SGD\n",
    "eval_number = 0\n",
    "trials = Trials()\n",
    "best_logreg = optimize(wc_framework, sgdlog_space, 20, trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests: single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dimensionality reduction\n",
    "#svd_dimred = TruncatedSVD(n_components=500)\n",
    "#train = svd_dimred.fit_transform(train)\n",
    "\n",
    "rf_space = {'type': RandomForestClassifier,\n",
    "            'n_estimators': 500,\n",
    "            'criterion': hp.choice('rf_crit', ['gini', 'entropy']),\n",
    "            'max_features': hp.choice('rf_maxfeat', ['sqrt', 'log2', None]),\n",
    "            'class_weight': hp.choice('rf_cweight', ['auto', 'subsample', None]),\n",
    "            'n_jobs': -1,\n",
    "            'scaling': hp.choice('rf_scaling', [None])\n",
    "            }\n",
    "\n",
    "# random forests\n",
    "eval_number = 0\n",
    "trials = Trials()\n",
    "best_rf = optimize(wc_framework, rf_space, 10, trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost: single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgbmulti_space = {'type': xgb.XGBClassifier,\n",
    "                  'n_estimators' : 200,\n",
    "                  'learning_rate' : hp.quniform('xgb_eta', 0.025, 0.5, 0.025),\n",
    "                  'max_depth' : hp.quniform('xgb_max_depth', 1, 31, 2),\n",
    "                  'min_child_weight' : hp.quniform('xgb_min_child_weight', 1, 6, 1),\n",
    "                  'subsample' : hp.quniform('xgb_subsample', 0.5, 1, 0.05),\n",
    "                  'gamma' : hp.quniform('xgb_gamma', 0.5, 1, 0.05),\n",
    "                  'colsample_bytree' : hp.quniform('xgb_colsample_bytree', 0.5, 1, 0.05),\n",
    "                  #'num_class' : 20,\n",
    "                  #'eval_metric': 'merror',\n",
    "                  'objective': hp.choice('xgb_objective', ['multi:softprob', 'multi:softmax']),\n",
    "                  'scaling': [None],\n",
    "                  'feat_sel': {'alg': SelectPercentile,\n",
    "                              'args': {'score_func': chi2,\n",
    "                                       'percentile': hp.quniform('prcnt', 5, 100, 5)}\n",
    "                             }\n",
    "                  }\n",
    "\n",
    "eval_number = 0\n",
    "trials = Trials()\n",
    "best_xgb = optimize(wc_framework, xgbmulti_space, 20, trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### To submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data:\n",
    "test_df = pd.read_json('raw-data/test.json')\n",
    "test = test_features\n",
    "\n",
    "# extracting best performing framework\n",
    "fmwk = best_xgb['result']['fmwk']\n",
    "\n",
    "# getting predictions\n",
    "predictions = fmwk.fit_predict(train, test, y_train)['preds']\n",
    "\n",
    "# saving pipeline:\n",
    "joblib.dump(fmwk, './models/sub10.pkl') \n",
    "\n",
    "# building submission\n",
    "submit = pd.DataFrame({'id': test_df['id'], 'cuisine': predictions})\n",
    "\n",
    "# decoding predictions\n",
    "with open('./raw-data/enc_dict.txt', 'r') as f:\n",
    "    enc_dict = ast.literal_eval(f.read())\n",
    "\n",
    "inv_map = {v: k for k, v in enc_dict.items()}\n",
    "submit = submit.replace({'cuisine': inv_map}).loc[:, ['id', 'cuisine']]\n",
    "\n",
    "# saving submission\n",
    "submit.to_csv('./submissions/sub10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission results\n",
    "\n",
    "1) something went wrong with SVD (maybe should use just .transform() on test set) ~0.68 CV; 0.23 LB (sub1) <br>\n",
    "2) raw data, tf-idf, SVM, SGD: ~0.734 CV; 0.74 LB (sub2) <br>\n",
    "3) raw data, tf-idf, log regression, SGD: ~0.671 CV <br>\n",
    "4) raw data, counts, SVM, SGD: ~0.720 CV <br>\n",
    "5) raw data, cuisinefreqs, SVM, SGD: ~0.724 CV <br>\n",
    "6) stemmed data, tf-idf, SVM, SGD: not better than (2) <br>\n",
    "7) stemmed data, tf-idf, SVM, SGD, 2-grams: 0.742 CV; 0.74206 LB (sub3) <br>\n",
    "8) stemmed data, tf-idf, bagged SVM (50 est), SGD, 2-grams: 0.748 CV; 0.74638 LB (sub4) <br>\n",
    "9) stemmed data, tf-idf, bagged SVM (50 est), SGD, 5-grams, min_df=4: 0.744 CV; <br>\n",
    "10) stemmed data, tf-idf, single XGBoost (100 est), 2-grams: 0.786 CV, 0.7855 LB (sub5) <br>\n",
    "11) stemmed data, tf-idf, single XGBoost (5000 est), 2-grams: 0.7857 LB (sub6) <br>\n",
    "12) stemmed data, tf-idf, single XGBoost (100 est), 5-grams (min=8) : 0.781 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
